hlfs 是什么？  
 hlfs （hdfs log structrue filesystem 的缩写）是一个在hadoop hdfs 文件系统之上实现的log structrue filesystem - like 系统。但要注意它并非一个实现完整posix语义的文件系统（支持目录操作、link等），而是仅仅实现了单一文件的基本管理（open,write,read,close)的系统，所以把它称为存储系统（block-level)可能更加合适。

hlfs 能做什么？ 
 由于hlfs自身特色（高可用性、写透、快照、分布式、用户态），适合于做虚拟机镜像存储(类似亚马逊EBS）、在线网盘（金山快盘等）等虚拟块设备的存储架构。甚至发散一下思维，它也可以做为pnfs类似系统的后台存储等等。 
 * 注， 其实我们的这次发布中有一个在nbd基础上实现的网盘，完全可以做为一个在线网盘使用。

hlfs 来源自哪里？
 hlfs 项目来自于我们当前正在研发的弹性云平台：基于erlang实现的一套虚拟机、虚拟网络管理系统，该系统有点像EC2，不过最终重点放在了管理虚拟子网（二层）上。这个项目也是一个开源项目，目前还处于开发初期。hlfs是该项目中的一个子项目，预先开源出来。

hlfs 如何实现的？
 简单的讲,hdfs 的实现特点决定了它完全可以当做一个高可用性、几乎无限大的网络磁盘使用，
但不幸的是，它只能追加写入，无法随机写入（其实正是这种取舍保证了hdfs反熵、故障切换等简单可靠的特点），对于在线应用随机访问而言（如虚拟机磁盘镜像）无法直接使用。所以我们借助log structrure filesystem的特色，使用追加方式的元数据管理技术，在hdfs这个大磁盘上实现随机数据读写功能。 具体的实现方法见设计描述  http://code.google.com/p/cloudxy/wiki/HlfsDesign?ts=1322292067&updated=HlfsDesign。

hlfs 有哪些特点？
 * hlfs 继承了hadoop dfs 和 log structrue filesystem的诸多优点，同时也继承了其缺点。
优点： 
 # 高可用性 —— 继承dfs
 # 高扩展性（可上大规模）、自动不停机扩容 —— 继承dfs
 # 负载均衡 —— 继承dfs
 # 用户态实现(便于调试、故障易隔离） ——继承dfs 
 # 稳定性、可管理性高 —— 得益于dfs成熟、普及
 # 写速度优化 —— 继承lfs 和 dfs
 # 支持运行态数据快照（便于数据污染后，执行回滚） —— 继承lfs 
缺点：
 # 需要数据合并（merge或者rewrite，已清除的旧数据回收存储空间）—— 遗传自lfs
 # 响应速度打折（副本一致性是通过pipeline式串行实现保证，所以写速度受一定影响） ——　遗传自dfs


为什么开源？
  # 集体智慧永远大于个人或者小团体智慧，因此希望能有更多朋友参与进来，贡献智慧，一同进步。
  # 饮水思源，从来都是用别人开源软件，好歹也应该尝试加入潮流。

hlfs roadmap ?
  目前0.1版本实现了基本概念原型，系统支持随机读写。未来将进一步实现:
  # checkpoint机制  ——
  # block cache机制、异步回写机制 —— 目前都是写透读透，安全性高，但性能差。
  # 预读readahead   —— 读性能优化主要靠readahead和提高cache命中率。
  # 异步实现hlfs driver for tapdisk —— 目前实现是用同步IO模型实现之。
  # hdfs一致性的改进（或者说特化） 

hlfs 项目中包含那些组件 ？
 * libhlfs 库
 * tools —— 
    # mkfs.hlfs 格式化工具
    # hlfs-seg-usaage-calc 段使用统计工具
    # hlfs-seg-clean 段清理工具（只能用于离线状态使用）
 * hlfs driver for tapdisk2 
    # hlfs driver tapdisk2的一个driver
    # tapdisk_ops 外部控制工具
 * 采用nbd架构的虚拟磁盘实现 
    # ndb-server
    # nbd-client
    # nbd-ops 外部控制工具


hlfs 和同类项目有什么区别 ？
 # 和sheepdog——牧羊犬项目的目的和hlfs最为相似，都是奔着EBS服务而生。hlfs先从xen虚拟化实现入手，sheepdog从kvm入手 （不过这都不重要，两者都是面向block-level的）。牧羊犬项目的实现，在我看来颇具创意——其采用对等网（无单点）结构实现后台存储服务，颇有Dynamo的影子在其中；hlfs的后台存储没有自己实现，而是建设于hadoop dfs之上，可以说是站在巨人肩膀上 ——　我们也曾经思考过独立开发分布存储系统，不过考虑工作量、以及更重要的是,考虑到我们设想是在最终理想的云环境中离线任务和在线应用是无差别的混合部署。而hadoop是离线应用的基石之一。所以希望整个系统都能遵循以hdfs为最下层存储基础，其他各种服务(如VM服务镜像存储服务、如key value存储服务、SQL \NO SQL等数据服务）都建立在其上的“分层策略”。基于此考虑，我们选择了hdfs做后台存储。
 # 和moose   ——有些公司采用moose实现虚拟机镜像存储（存放镜像文件）。moose系统做镜像存储来说有点“大材小用”，或者尾大不掉的感觉。它是一个实在的文件系统，而不是block-level级别的存储系统，相比而言元数据管理稍微重点。moose系统存储镜像，就需要使用fuse模式挂载到本地系统，也就是说数据流要经过内核态转发，相比hlfs只存在于用户态，所以数据流更精简，另外就是错误不至于影响内核错误；另外moose系统的反熵过程比较慢，较为影响当前写操作；不能实现随时快照功能。
 # 和Drbd    ——DRDB可看成网络RAID。能解磁盘冗余、跨机器问题。但不能提供thin provisoning，也不能动态负载均衡。