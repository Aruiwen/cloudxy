#summary HLFS用户手册.

= HLFS 用户手册 =

== 准备动作 ==
基本环境要求：
 * 32、64位Linux系统操作系统（我是在ubuntu10.4和Centos上进行开发的）
 * 2G  以上内存
===Hadoop 安装===
   hlfs需要使用hadoop hdfs的append功能实现log追加操作，因此我们需要选择hadoop-0.20-append或者cloudera的ch3版本（默认支持了append功能，不用再配置 dfs.support.append=true了，如果使用其他版本一定要打开该功能），关于hadoop append问题域的分析请见http://www.cloudera.com/blog/2009/07/file-appends-in-hdfs.<br>
   我们建议使用cloudera的ch3，安装方式参见  https://ccp.cloudera.com/display/CDHDOC/CDH3+Installation
===Xen 安装===   
   我们使用xen中的tapdisk2模式加载hlfs磁盘，因此需要使用xen4.0之上版本（如果低版本xen需要打上tapdisk2的patch)。因此我们建议使用xen4.1.x 。 对于dom0内核版本，我们建议不要用2.6.18的老内核，使用当前2.6.32以上的内核比较可靠。<br>
   关于xen的安装，网上资料很多。 也可以参见我们download中提供的资料http://code.google.com/p/cloudxy/downloads/detail?name=xen_guide.pdf
   

== HLFS START UP ==
 === 获取hlfs代码和相关依赖库 ===
   在终端键入'svn checkout http://cloudxy.googlecode.com/svn/trunk/hlfs/  hlfs' （因为需要包含了第三方库缘故，因此肯恩checkout时间比较长）
 === 编译hlfs ===
   # 安装cmake;安装glib库;安装java。
   # 修改在hlfs/src/CMakeLists.txt中配置你正确的java目录 'SET(JAVA_HOME   /usr/lib/jvm/java-6-sun)'。
   # 生成makefile:cd hlfs/build；cmake ../src（生成makefile)
   # make {all | libhlfs | tools | test} 
      * all: 生成所有库和工具
      * libhlfs: 生成hlfs动态库
      * tools: 生成所有工具
{{{
   1. src hlfs库源代码目录_
   2. nbd-2.9.15 修改后的nbd-server/nbd-client源代码目录
   3. 3part 第三方库，目前包含hadoop相关;log库
   4. output 编译后的生成库和工具所在目录
   5. patches hlfs driver for blktap2的patch代码
}}}
 === 测试hlfs ===
 为了方便调试目的，hlfs实现中支持了本地测试（不需要hadoop hdfs),我们称为local模式。我们首先用local模式测试
   * 编译hlfs - cd hlfs/build;cmake ../src/;make all
   * local模式下格式化hlfs文件系统 
       # mkdir /tmp/testenv //需要一个类似workshop的工作环境
       # ./output/bin/mkfs.hlfs -u local:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024 (-b是数据块大小，单位是byte， -s是段文件大小，单位是byte， -m 是虚拟磁盘的最大容量,单位是M) 
   * 编译测试用例 - cd test;camke .;make 
   * 执行测试用例 - ./test -u local:///tmp/testenv/testfs 
{{{
    * 格式化目的是写入原数据到superblock中，这些元数据描述了该hlfs的属性信息；当格式化完成后，你会在/tmp/testenv目录下，发现testfs目录，再其下发现superblock文件，查看该文件，会发现块大小、段大小、容量限制等该hlfs的属性信息
    * 执行完毕后，会在/tmp/testenv/testfs下发现生成的段文件
}}}
 如果你已经安装hadoop hdfs，并且已经启动，则可以测试hdfs模式，测试方式
  * 编译hlfs - cd hlfs/build;cmake ../src/;make all
  * hdfs 模式下格式化hlfs文件系统 (我们以伪分布模式执行）
       # hadoop fs -mkdir hdfs:///tmp/testenv
       # ./output/bin/mkfs.hlfs -u hdfs:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024
  * 编译测试用例 - cd test;camke .;make 
  * 执行测试用例 - ./test -u hdfs:///tmp/testenv/testfs   
{{{
  * 可见local模式和hdfs模式只是uri的head不同而已，其余用法类似
  * hdfs模式uri支持
     1. hdfs:///tmp/testenv/testfs                       //hdfs伪分布模式下使用
     2. hdfs://192.168.0.1/tmp/testenv/testfs            //默认端口为8020
     3. hdfs://192.168.0.1:8020/tmp/testenv/testfs       //完整uri；192.168.0.1也可是主机名
  * 执行hadoop fs -ls /tmp/testenv/testfs命令你可看到类似local模式下的各种文件(hadoop shell 命令请见http://hadoop.apache.org/common/docs/r0.17.2/hdfs_user_guide.html)   
  * 说明一下hadoop/libxx/libhdfs.so库并非标准ch3代码编译而得，因为标准代码没有支持hflush操作，为了支持该操作，我再ch3源代码上打了hflush补丁（patch请见xxx)。 
}}}

== HLFS 存储虚拟机镜像 == 
如果上述步骤成功，那么可以开始尝试EBS了。使用HLFS作为虚拟机的数据盘，甚至可以做系统安装盘。
   * 添加hlfs driver for blktap2
     # 进入xen/tools目录
     # 打上patches目录下的hlfs_driver_for_blktap2.patch - 执行patch -p1 <　patches/hlfs_driver_for_blktap2.patch
   * 编译blktap2，安装blktap2，激活blktap2
     # 在tools目录下执行make clean;make;make install
     # 重启动xend - service xend restart
   * 测试hlfs driver for blktap2
     # local模式测试- mkfs.hlfs -u local:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024；tap-ctl create -a hlfs:local:///tmp/testenv/testfs；
     # hdfs模式测试-  mkfs.hlfs -u hdfs:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024；tap-ctl create -a hlfs:hdfs:///tmp/testenv/testfs
   * 作为DomU的数据盘使用-EBS用法。
     # 可以使用动态挂载 如 xl block-attach 0 tap2:hlfs:///tmp/testenv/testfs xvda rw 
     # 通过配置文件配置 如 disk = ['tap2:aio:root/tapdisk-test/centos-mother.img,xvda,w','tap2:hlfs:local///tmp/testenv/testfs,xvdb,w']
   * 作为DomU的系统盘使用
     # 直接使用生成的tapdev设备（如/dev/xen/blktap-2/tapdev0），做类似phy磁盘的使用。
     # 使用生成的tapdev设备，格式化后，存储vhd镜像文件，再使用vhd镜像文件启动。
== HLFS 用作虚拟网盘 == 
  * 编译nbd  
     # cd nbd-2.9.15/build;
     # cmake ../;
     #  make (编译后内容在nbd-2.9.15/outpub下）
  * 加载模块 - sudo modprobe nbd (nbd服务需要一个对应的内核模块，这个模块一般没有被默认加载）
  * 启动nbd server - ./nbd-2.9.15/output/bin/nbd-server 20000 (20000是服务端口号）    
  * 格式化hlfs - 如上所示 （如 mkfs.hlfs -u hdfs:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024；mkfs.hlfs -u local:///tmp/testenv/testfs -b 8192 -s 67108864 -m 1024)
  * 挂接nbd设备 - 将nbd设备和上面格式化的fs关联（如./nbd-2.9.15/output/bin/nbd-client bs=512 127.0.0.1 20000 local:///tmp/testenv/testfs /dev/nbd0；./nbd-2.9.15/output/bin/nbd-client bs=512 127.0.0.1 20000  hdfs:///tmp/testenv/testfs /dev/nbd0） 
  * 测试nbd设备
     # sudo mkfs /dev/nbd0
     # sudo mount /dev/nbd0 /opt
     # do everything like a local device

{{{
我们对传统nbd server和 nbd client做了一定修改_
 1. nbd-server 只需要给出端口号，不用给出其他信息了
 2. nbd-client 要给出对应fs的uri地址——当和server握手时传给对方，server 再根据该uri初始化hlfs上下文，然后通过hlfs_stat获得fs大小信息，并传给client段
 3. 我们还增额外实现了nbd-ops工具用于控制hlfs的merge行为（具体见高级用法），外部控制采用管道通讯+共享内存实现，具体请见代码
    
   另外nbd还有一个讨厌的问题 —— nbd 死锁问题值得注意 :
   nbd-server 如果和nbd-client如果部署在统一个机器上，在某些内核上会出现IO hang住（死锁）的问题。追其原因是：当系统内存不足时，必然需要pdflush例程将脏数据刷到磁盘已回收可用内存，而引发内存分配的肇事进程则必须同步等待上述释放动作，然后再获取刚回收来的内存继续运行。
   那么可以想象当nbd-server运行时需要内存，但系统空闲内存不足时，就会触发pdflush的刷新操作，这是要被写入脏页的磁盘又是自己时（因为nbd盘作为数据盘也要被写入脏页的），则会出现死锁——因为要靠nbd-server继续运作才能将脏页写下去，而nbd-server运作的前提又是需要获得内存。
   不过并非所有内核都会有这个问题（2.6.18肯定是有,新内核上后来引入pdi概念、pdflush刷新方式改变等使得死锁问题得以消除），我使用的ubuntu 2.6.35的内核就没有次问题。因此如果要在本地尝试nbd，最好使用高版本内核，或者内存给的大大的。实在不行，就麻烦你就去修改一下内核把：没记错的话，大概是在bdi_congestion处做一下手脚，让其避开nbd-server这个进程好了:)
}}}


== 旧数据merge功能 ==
从设计文档中可以看到hlfs需要使用merge功能消除旧数据。而我们将merge过程分为俩步走：
 # 段统计：计算当前时刻的段活跃数据块数量，为下一阶段回收做一定预先判断（注意只是预判断，当回收任务执行时，段中活跃数据块数据有可以变化）。
 # 回收段：对于那些活跃数据块小于给定阀值，但尚未为0的段，搬迁其依旧活跃的数据段（相对回收当前时间）到新段，然后在回收该段；如果活跃数已经为0，则直接回收该段。
=== 段统计 ===
 # 采用拉模式：即在通过本地客户端方式执行 — segcalc.hlfs -u uri （在uri所在目录升到成段统计文件）
 # 采用推模式: 即采用map/reduce模式，将该任务下发到段所在机器执行，然后将段统计值用reduce方式写到段统计文件中。具体做法如下：
  # 编译map/reduce执行程序 - cd hlfs/src/clean/Mapreducer/build;cmake .l;make
  # 将上述执行程序上传到hfds下指定目录 - hadoop fs -put hlfs/src/clean/Mapreducer/outpub/bin/mr_segcalc.hlfs /exe/
  # 下发map/reduce job - hadoop pipes -D mapred.reduce.tasks=1 -input /tmp/testenv/testfs  -output /output -program /exe/mr_cleaner
{{{
采用推模式map/reduce实现段统计可发挥分布系统并行处理优势，但会带来额外的网络负担。
关于C++ pipe实现map reduce程序请见pipe的例子代码。还有http://wiki.apache.org/hadoop/C%2B%2BWordCount#Upload_C.2B-.2B-_binary_files_to_HDFS等资料。
}}} 

===段回收===
 * 离线情况下，standalone方式运行段回收
   seg.clean -u uri -s segno -w waterlevel （如果segno是-1， 那么段清理将从第一个段开始，依次往后进行清理工作；waterlevel是回收阀值，单位是数据块，如果段中活跃数据块小于waterlevel则，执行清理工作）
   
 * 在线情况下，运行段回收。
   在线情况下，不能使用上述命令直接跑段回收任务，否则会造成数据破坏。在线情况下，段回收目前设计在hlfs的写入线程中实现。段回收任务相比写入请求而言，属于低优先级，因此只有当不再有写入请求的情况下，才会开始执行。
 # tapdisk2 开启段回收
   outpub/bin/ -p pid -u uri -c cmd -r parameter -v verbose (-p后跟正在运行的nbd主进程号， -u后跟文件系统路径，-c后跟要执行的命令，包括start_merge：段回收开启；stop_merge：段回收停止；set_copy_waterlevel：设置回收阀值；query_stat：查询状态,r后面跟着相应命令所对应的参数，-v是是否输出详细的运行信息)。
 # nbd-server 开启段回收
   outpub/bin/tapdisk_ops -f fsname -c cmd -r parameter -v verbose (-f后跟着文件系统名称，-c后跟要执行的命令，包括start_merge：段回收开启；stop_merge：段回收停止；set_copy_waterlevel：设置回收阀值；query_stat：查询状态, -r后面跟着相应命令所对应的参数，-v是是否输出详细的运行信息)。	
{{{
   对于hlfs库来说，
   1.hlfs_clean_start(HLFS_CTRL *ctrl);
   2.hlfs_clean_stop(HLFS_CTRL *ctrl);
   3.int  hlfs_set_clean_level(HLFS_CTRL *ctrl,unsigned int alive_bytes);
   等几个函数对应了上述命令的最终调用。
   nbd-server和tapdisk2的外部控制命令实现不尽相同，tapdisk2用的更为简单，nbd-server实现复杂些，但更灵活。因此两种方式都保留，供大家参考。
}}} 

===关于日志===
hlfs支持log4c日志系统，但作为库，日志系统的初始化工作并不会去做。如果需要使用请在调用方执行log4c_init()


===关于bug===
由于人力不足（目前只有我和贾威同学），水平有限，因此内部很多功能测试不完善，留有不少bug，因此请见谅。
如果您有兴趣试试，发现bug,欢迎提给kanghua151@msn.com 或者 cloudxy@googlegroups.com(我们的邮件列表），更欢迎直接修改ci。 
谢谢
                         康华 
                         2011.11.27 晚